{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db2de2e",
   "metadata": {
    "id": "2db2de2e"
   },
   "source": [
    "# Week 3: Introduction to Data Science ðŸ“ˆ\n",
    "# Pre-module\n",
    "\n",
    "Now that you are well-versed in a new language (Python), you are ready to take a deeper look at the dataset collected by your colleagues. This week's modules will introduce you to data science tools that you will access and use through the Python language.\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "In this pre-module activity, you will learn how to:\n",
    "1. Load data in a CSV file into a `pandas` DataFrame using `read_csv()`\n",
    "2. Understand the `DataFrame` datatype and its shape/axes\n",
    "3. Check for and remove rows with missing values using `isna()` and `drop()`/`dropna()`\n",
    "2. Export a `pandas` DataFrame as a CSV file using `to_csv()`\n",
    "\n",
    "### Heart failure dataset\n",
    "\n",
    "Recall from previous weeks that your research group is interested in identifying risk factors for death in heart failure patients. Your team has compiled clinical records of patients with previous records of stage III/IV heart failure, which were provided by several physicians whose patients were willing to participate in the study, into **hf_data_raw.csv**. Take a few minutes to open this file in Excel and take note of the information collected.\n",
    "\n",
    "Although your team of researchers has taken great care to provide physicians with clear instructions for recording patient data, sometimes mistakes find their way into the dataset, or some parts of the dataset may not be useful for our purpose. With hundreds of patients participating in the study, it is impossible for you to sift through each patient's data manually to ensure the dataset is clean for analysis. Your first task is to take a look at the dataset as-is, identify any obvious issues, and make any modifications required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e601dc5",
   "metadata": {
    "id": "6e601dc5"
   },
   "source": [
    "### Loading a CSV file\n",
    "First, we must be able to inspect the dataset using code, rather than in Excel or other software for managing tabular data. Although Excel is a useful tool for data organization and analysis, data scientists can encounter much larger datasets that call for faster, more powerful tools. One of the main tools we will be using in this course is the `pandas` library.\n",
    "\n",
    "The syntax for importing a Python library, which you may have seen in weeks 1 and 2, is:\n",
    "\n",
    "`import <library name> as <alias>`\n",
    "\n",
    "The `alias` is not necessary, but using shorthands for long library names helps improve readability, as we will call upon these libraries very frequently throughout our exploration. Some aliases are also widely accepted in industry and the computing community to facilitate better collaboration and communication. For example, `pd` is the widely accepted alias for `pandas`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430efc7a-1122-4ce6-8d8f-fc5282e00991",
   "metadata": {},
   "source": [
    "---\n",
    "**Q*1.Let's import the `pandas` library with `pd` as the alias.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db0c20",
   "metadata": {
    "id": "22db0c20"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Ot4wtTtSW-UR",
   "metadata": {
    "id": "Ot4wtTtSW-UR"
   },
   "source": [
    "---\n",
    "The `read_csv()` function in `pandas` takes in a CSV file and returns a DataFrame. `DataFrame` is a datatype for tabular data.\n",
    "\n",
    "`read_csv()` has one mandatory parameter: the path to the CSV file you wish to convert to a DataFrame. Upload the file hf_data_raw.csv to JupyterHub in the same folder that this module is in, if you have not already. Assuming that this file is in the same folder as this Jupyter Notebook, the filepath should be the name of the dataset file as a string (i.e., wrap the name in quotation marks).\n",
    "\n",
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| read_csv() | filepath | pandas DataFrame | read_csv(filepath) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a1c7a-254d-4359-9c2c-2fd9e17c8443",
   "metadata": {},
   "source": [
    "---\n",
    "**Q*2. Try it out -- read hf_data_raw.csv in as a `pandas` DataFrame.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2clc4ojdW-UR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 251,
     "status": "error",
     "timestamp": 1732656338941,
     "user": {
      "displayName": "Elias Williams",
      "userId": "15612737538936993151"
     },
     "user_tz": 300
    },
    "id": "2clc4ojdW-UR",
    "outputId": "90620017-0626-4345-cb7f-527525018aca"
   },
   "outputs": [],
   "source": [
    "# Complete this line\n",
    "df = ...\n",
    "\n",
    "# Let's see what the DataFrame looks like\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403azgl5W-US",
   "metadata": {
    "id": "403azgl5W-US"
   },
   "source": [
    "---\n",
    "Great! You should be able to see some rows and columns of the DataFrame. You'll notice that this dataset is probably too big to show here, hence the ellipses. Other observations about a DataFrame:\n",
    "\n",
    "1. DataFrame column labels correspond to the first row of the CSV file, and are strings (text) in this case;\n",
    "2. DataFrame rows are numbered 0...num rows - 1 (i.e., they are 0-indexed).\n",
    "\n",
    "There is a way to find out the number of rows and columns without printing `df` and observing the last row index. These are important parameters because if we want to access particular rows and/or columns later on, we need to have a sense of the maximum index we can access without going out of bounds (which will throw an error).\n",
    "\n",
    "**Run the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SnYgsi86W-US",
   "metadata": {
    "id": "SnYgsi86W-US",
    "outputId": "5e67d286-ce17-45f1-8a11-3fa3ce778d08"
   },
   "outputs": [],
   "source": [
    "print(\"Shape: \", df.shape)\n",
    "rows = df.shape[0]\n",
    "cols = df.shape[1]\n",
    "print(\"Number of rows: \", rows)\n",
    "print(\"Number of columns: \", cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R2gRul-MW-UT",
   "metadata": {
    "id": "R2gRul-MW-UT"
   },
   "source": [
    "<span style=\"background-color: #AFEEEE\">**axis:**</span> a dimension of the DataFrame. Since a DataFrame is 2-dimensional (has rows and columns), there are 2 axes, 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f861870c-b14a-47dd-bb6b-7f1b0e215155",
   "metadata": {},
   "source": [
    "---\n",
    "**Q*3. For this dataset, which axis (0: rows, 1: cols) is features? Which axis is samples (individual patients)?**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your answer below**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Oqp1B0oAW-UT",
   "metadata": {
    "id": "Oqp1B0oAW-UT"
   },
   "source": [
    "Answer here: \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wcvE_a2mW-UT",
   "metadata": {
    "id": "wcvE_a2mW-UT"
   },
   "source": [
    "We can use `head(n)` and `tail(n)` to display the first and last n rows of the dataset, respectively.\n",
    "\n",
    "**Run the two code cells below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nzZ3XeQdW-UT",
   "metadata": {
    "id": "nzZ3XeQdW-UT",
    "outputId": "034056ac-8abf-4229-9bce-9714a0e63ca9"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g5RvSmZ_W-UT",
   "metadata": {
    "id": "g5RvSmZ_W-UT",
    "outputId": "47640657-4599-49fe-f805-02b085e09b7b"
   },
   "outputs": [],
   "source": [
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kWkNTlbPW-UU",
   "metadata": {
    "id": "kWkNTlbPW-UU"
   },
   "source": [
    "As we can see, a `pandas` **DataFrame** has rows and columns of data; it presents data as a table. Each row and column is a `pandas` **Series**, which is basically like a list (covered in week 1). However, these are different from basic Python lists in that `pandas` provides us with functions that can be specifically called on Series. Likewise, there are functions that we can call specifically on DataFrames.\n",
    "\n",
    "Remember from week 1 that we can access elements of a list by indexing. Now that we have a DataFrame, which is essentially a table of elements, how do we access rows, columns, and individual elements?\n",
    "\n",
    "To get a whole column, we do `df['column name']`. This is the same syntax you saw from week 2, when you learned about dictionaries.\n",
    "To get a whole row, we do `df.iloc[row index]`. `iloc[]` is a function that works on a `pandas` DataFrame that allows us to access parts of the DataFrame by index. Aside from having to use `iloc[]`, it works the same as indexing into a list.\n",
    "\n",
    "**Run the code cell below. Look at the output of the 5th row printed below, and the last row in the output of `df.head(5)` above, to see that they are referring to the same row.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5CyFpe42W-UU",
   "metadata": {
    "id": "5CyFpe42W-UU",
    "outputId": "7b480d5c-22fd-4e4f-adf0-d8787aef7e19"
   },
   "outputs": [],
   "source": [
    "# Print the age column\n",
    "print(\"Print age column:\")\n",
    "print(df['age'])\n",
    "\n",
    "# Print the 5th row (index 4)\n",
    "print(\"\\nPrint the 4th row:\")\n",
    "print(df.iloc[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aa2743",
   "metadata": {
    "id": "37aa2743"
   },
   "source": [
    "### Data cleaning\n",
    "\n",
    "Let's think about what data cleaning involves. What needs to be \"cleaned\" will vary from dataset to dataset, but here are a few good starting points.\n",
    "\n",
    "The most obvious thing we should look for is rows with missing values. Either the data collector (physicians in this case) has made an error, or the information was not available. In a DataFrame, an entry at the i'th row and j'th column is empty or missing if it is **NaN** (Not a Number). Missing values can cause issues later when we compute statistics or apply machine learning models. For this exercise, we will remove rows with missing values, but it is possible to retain such rows through data imputation (see Further Reading at the end of this module).\n",
    "\n",
    "The DataFrame function `isna()` returns a DataFrame with boolean values indicating whether the entry at that position is missing or not. It converts every data point into a boolean, so the resulting shape should be the same as your DataFrame.\n",
    "\n",
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| isna() | n/a | A DataFrame of boolean values (whether the element is missing or not) | df.isna() |\n",
    "\n",
    "**Run the code cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bffc6",
   "metadata": {
    "id": "8c9bffc6",
    "outputId": "d9fe9b7f-6e06-4d1f-f343-6b6446f55afb"
   },
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d9796",
   "metadata": {
    "id": "102d9796"
   },
   "source": [
    "We can't possibly look through every row in this format. For all we know, maybe there aren't any missing values in the entire dataset. We want to see only those rows with missing values.\n",
    "\n",
    "The function `any()` returns whether any element in the set is `True`. This function can be called on an axis (row or column) in a DataFrame. If we call `any()` on the column axis, the function will check every column of a row and return `True` for that row if any of the columns are `True`. This would be done for all rows, so that the final result is a `True`/`False` value for each row.\n",
    "\n",
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| any() | axis | The DataFrame, with the given rows/columns dropped. | any(axis) |\n",
    "\n",
    "By using `any()`, we reduce the above result to a single dimension, which helps us extract just the information we need.\n",
    "\n",
    "**Run the code below to see which rows have missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc669f34",
   "metadata": {
    "id": "bc669f34",
    "outputId": "5dc8c388-6938-432d-ace4-77eb9224804c"
   },
   "outputs": [],
   "source": [
    "list_nan = df.isna().any(axis=1)\n",
    "list_nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ba51a",
   "metadata": {
    "id": "f01ba51a"
   },
   "source": [
    "---\n",
    "**Q*4. We would like to see which columns have missing values, using a similar method as above with `isna()` and `any()`.**\n",
    "> Hint: You will only have to change the argument in `any()`\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355d64fe",
   "metadata": {
    "id": "355d64fe",
    "outputId": "3fa5491b-9117-4c57-b5de-51d3de22112c"
   },
   "outputs": [],
   "source": [
    "df.isna().any()  #complete this code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d93064e",
   "metadata": {
    "id": "9d93064e"
   },
   "source": [
    "---\n",
    "**Q*5. We can't possibly look through all 303 rows in `list_nan` above. But we can leverage a `pandas` feature called boolean indexing, which lets us filter a DataFrame with a list of booleans to return just the samples where the filter is `True`. We do this by using the boolean list as we would use an integer to index into a DataFrame, as such: `data_frame[boolean_list]`. Complete the code below using `list_nan` to return just the rows with NaN values.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4e5ad",
   "metadata": {
    "id": "8bb4e5ad",
    "outputId": "313517c9-84a5-474d-f71e-31f26113a6fb",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26482942",
   "metadata": {
    "id": "26482942"
   },
   "source": [
    "---\n",
    "Great! Now we can easily see that there are two patients with missing values; patient 36 is missing high blood pressure and smoking information, and patient 131 is missing their serum_creatinine measurement. How do we remove these patients from the dataset? Like most concepts we have covered so far, there are multiple ways to achieve the desired output.\n",
    "\n",
    "There are two main ways to drop rows in `pandas`: `drop()` and `dropna()`. We will discuss `drop()`, but you can read about `dropna()` in Further Reading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53080c2f-8fe4-41f0-b873-a3027e1b0ded",
   "metadata": {},
   "source": [
    "---\n",
    "#### **`drop()`**\n",
    "This function lets you drop rows by index. We figured out above that patients with index 36 and 131 need to be dropped.  \n",
    "\n",
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| drop() | labels (index or column labels to drop), axis | The DataFrame, with the given rows/columns dropped. | df.drop(labels, axis) |\n",
    "\n",
    "* `labels`: The row indices/column names indicating what you want to drop. If you want to drop multiple rows, the value you pass in for labels should be a list. In this case, your list should contain the indices 36 and 131.\n",
    "* `axis`: The axis to drop from. If 0, rows indicated by indices will be dropped. If 1, columns indicated by labels will be dropped. By default, the `axis` is 0 (so if you do not specify this parameter when you call `drop()`, the function will drop rows.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f08ca-309a-4345-be6e-fb6c48d24135",
   "metadata": {},
   "source": [
    "---\n",
    "**Q*6. Let's remove rows 36 and 131 and assign them to the variable ```df_drop```. You will need to call ```drop()``` on the variable ```df``` with the correct arguments.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245bd49",
   "metadata": {
    "id": "b245bd49",
    "outputId": "e5ab711f-dbd8-47a5-d558-b9459e9c5d60"
   },
   "outputs": [],
   "source": [
    "df_drop = ...\n",
    "df_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a9cdd-80e0-4b3a-86eb-eff8d427d0a3",
   "metadata": {},
   "source": [
    "Great! You have successfully removed rows with empty values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381ac9c",
   "metadata": {
    "id": "c381ac9c"
   },
   "source": [
    "### Exporting a DataFrame as CSV\n",
    "\n",
    "Now that we have our clean (as far as we can see) dataset, we will convert it back to a .csv file. We do this to save it in an easily shareable format, which we can load into any notebook session in the future. You will be using this clean dataset for your main module during your tutorial.\n",
    "This is achieved using the DataFrame `to_csv()` function.\n",
    "\n",
    "| Function | Input parameters | Output | Syntax |\n",
    "| --- | --- | --- | --- |\n",
    "| to_csv() | filename, index | n/a; saves the file to the same directory that your notebook is in | df.to_csv() |\n",
    "\n",
    "* `filename`: The name that you want the output file to be saved as. This should be a string. For example, \"output.csv\".\n",
    "* `index`: Specifies whether to save the row indices (0..max) as the first column. The default is `True` (but we will set this to `False` as this extra column is redundant for our purpose).\n",
    "\n",
    "**Run the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26515c",
   "metadata": {
    "id": "5b26515c"
   },
   "outputs": [],
   "source": [
    "df_drop.to_csv('df_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad0c1b",
   "metadata": {
    "id": "5aad0c1b"
   },
   "source": [
    "Check to see that this new csv file appears in your JupyterHub, in the same folder that this notebook is in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0XdWz6uaMxiD",
   "metadata": {
    "id": "0XdWz6uaMxiD"
   },
   "source": [
    "## **Graded Exercise: (1 mark)** \n",
    "\n",
    "**GQ*1. (1 mark) Write a line of code below that prints out the value of the 10th row of column \"platelets\" in `df_drop`.**\n",
    "\n",
    "<span style=\"background-color: #FFD700\">**Write your code below**</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yA1wiNmQNZtS",
   "metadata": {
    "id": "yA1wiNmQNZtS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a721973a",
   "metadata": {
    "id": "a721973a"
   },
   "source": [
    "## Conclusion\n",
    "In this pre-module activity, you have learnt to:\n",
    "1. Load data in a CSV file into a `pandas` DataFrame using **`read_csv()`**\n",
    "2. Understand the `DataFrame` data type and its shape/axes\n",
    "3. Check for and remove rows with missing values using **`isna()`** and **`drop()`**/**`dropna()`**\n",
    "2. Export a `pandas` DataFrame as a CSV file using **`to_csv()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rdwJxBDhW-UV",
   "metadata": {
    "id": "rdwJxBDhW-UV"
   },
   "source": [
    "## Further reading\n",
    "* Data imputation: https://medium.com/godlessindian/imputation-methods-in-data-preprocessing-b225f7456de1\n",
    "* `Pandas` **`isna()`** documentation: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html\n",
    "    * Note: **`isnull()`** is an alias of **`isna()`** and can be used interchangeably.\n",
    "* **`dropna()`**\n",
    "    * As an alternative of `drop()`, `dropna()` condenses the steps to drop rows with empty values. We can specify the scheme through the **`how`** parameter. Options for this parameter include:\n",
    "        * `'any'`: If at least one value is empty in the row/col, drop that row/col.\n",
    "        * `'all'`: If all values are empty the row/col, drop that row/col.\n",
    "    * We should also specify the `axis` that we want this dropping scheme to apply to. We want to drop *rows* with empty values, so we would specify `axis = 0` in the parameters.\n",
    "    * For our example, `df_dropna = df.dropna(how='any')` or `df.dropna(axis=0, how='any')` would work instead of the code we wrote for `drop()`.\n",
    "    * Documentation for this function: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html#pandas.DataFrame.dropna"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
